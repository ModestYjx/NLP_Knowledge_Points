[TOC]

# 预训练语言模型发展

## 1.预训练语言模型诞生

### 1. AR与AE语言模型

* AE：自编码（Autoencoding Language Model）通过下上文来表示当前词向量，代表模型有BERT, Word2Vec(`CBOW`)。由于其训练的特点。
  $$
  p(x) = \prod_{x\in Mask} p(x|context)
  $$

  ![](https://pic1.zhimg.com/v2-ac5de7890849432e16681a881fea5e50_r.jpg)
  
* AR：自回归编码（Autoregression Language Model）通过上文或者下文来表示当前词向量，即根据前面（或者后面）的token预测当前的token，代表模型有ELMO, GPT等。

$$
forward:\,p(x)=\prod_{t=1}^{T}p(x_t|x_{<t})\\
backward: p(x) = \prod_{t=T}^1 p(x_t | x_{>t})
$$

​	![](https://pic3.zhimg.com/80/v2-4f6492d8b4303d278441bbb4c933a20a_720w.png)

两者的优缺点：

* AR语言模型：
  * 优点：文本序列**联合概率的密度估计**，即为传统的语言模型，天然适合处理自然生成任务，**对生成模型友好，天然符合生成式任务的生成过程，这也是GPT能编故事的原因**
  * 缺点：联合概率按照文本序列从左至右分解（**顺序拆解**），无法通过上下文信息进行双向特征表征,**它只能利用利用单向语义而不能同时使用上下文信息（这种“废话”也能写！！！，要学会表达，一句话变成多句话说，把特点说成缺点），ELMO通过双向都做AR模型，然后拼接，但是从结果上看，效果并不是太好。**
* 代表模型：ELMO GPT
  * 改进：XLNet将传统的自回归语言模型进行推广，将顺序拆解变为**随机拆解**（排列语言模型），产生上下文相关的双向特征表示
  
* AE语言模型：
  * 优点：本质为降噪自编码特征表示，通过引入噪声[MASK]构建MLM，获取上下文相关的双向特征表示；**能够很好的编码上下文语义信息，（把特点当做优点）**适合于自然语言理解问题。
  * 缺点：由于训练中采用了[MASK]标记，导致预训练与微调不一致的问题，预训练时的[MASK]噪声在finetune阶段不会出现，造成两阶段不匹配问题，**此外对于生成式问题，AE模型也捉襟见肘（别人的优点就是它的缺点），这也是Bert为数不多实现大的突破的领域。**
  * 代表模型：BERT系列模型

### 2. Feature-base pre-training: Word Embedding 到ELMO

**考虑到词向量不能解决词的多义性问题**`（具体的一个词不是对应固定的向量表示吗？那多义性问题怎么可能直接解决？就像人一样，你看到“苹果”这个词，没有上下文，人也不知道指的是水果苹果还是苹果手机）`，**在ELMO之前，我们往往采用双向LSTM来缓解这个问题，然而治标不治本，对于大数据集来说，深层双向LSTM可以很好的缓解这个问题，但对于小数据集，往往没啥效果。**

**为了解决多义性问题，ELMO采用了深层双向LSTM模型。不同层的LSTM能把握不同的粒度和不同层次的信息，比如浅层LSTM捕获单词特征，中层LSTM捕获句法特征，深层LSTM把握语义特征，对不同任务，不同特征起了不同作用。**

**举例来说，对于文本分类问题，ELMO和BERT与Word2Vec相差无几，因为对于文本分类问题，n-gram信息起了很大的作用，而这本质就是单词特征；但对于阅读理解，ELMO与BERT就能大幅度提升性能，这也是因为语法与语义特征对阅读理解这种深层次问题是十分重要的。**

**ELMO在迁移到下游任务时，会将不同层的特征用加权求和的方式来获得每个词的最终表示。**

**事实证明**`（怎么个证明方法？通过设计特定的任务吗？比如让该模型进行完形填空？）`，**ELMO的确解决了多义性问题，词性也能对应起来了。**

但ELMO的缺点也十分明显：

* **LSTM特征抽取能力远弱于Transformer，并行性差**
* **拼接方式双向融合特征融合能力偏弱**

### 3. Fine-tuning pretraining:  GPT的诞生

![](../img/pretrain_fineturning.png)

pretrain与fine-turning的区别，见上图

虽然GPT不是第一个预训练模型，但是其仍具有开创性意义，其特点很明显：

* **采用Transformer作为特征提取器**
* 采用二阶段：预训练 + 微调 来适配下游任务`(预训练模型不都是这样吗？这算明显特点)`

**GPT1.0和GPT2.0的出现说明了以下几点：**

* 深层的Transformer模型具有更强的表示能力
* **高质量，大规模的预训练数据集是提升性能的根本**

**至少，从目前为止，业界还没有探索到数据与模型的极限，即仅仅堆数据，加深模型这条路，还没有走完。**`(数据集可以无止境增加，模型可以无限加深，但要通过残差，归一化等技术来控制调优)`

### 4. 预训练新时代： BERT

GPT虽然很强，但其是基于AR的语言模型，目前很多排行榜都是基于自然语言理解的，因此这方面，GPT无法与BERT抗衡。但GPT比BERT更加会编故事。

BERT主要分为两大任务：Masking和NSP(next sentence prediction)`（微调阶段也进行这两个任务吗？）`

BERT由于采用AE模型，MASK操作所带来的缺陷依旧存在：

* 预训练与微调阶段不匹配的问题`(没明白，举个具体的例子)`，**这点BERT提供了一个策略来减轻该问题。**`（什么策略？）`

* **Mask掉的token之间关系被忽略的问题**

此外，由于数据量、模型都十分大，如果每次只对一个token进行mask会导致训练时间过长，文章采用**masking 15%**的操作，这是一个经验性的选择，是对模型训练效果与训练时长做出的一个权衡。

**至于NSP任务，事实证明其在句子关系上的确起到了一定的作用，对于某些任务的确有帮助，但也有文章指出，其实用处不大。**

## 2. BERT之后的改进方案

BERT之后，有很多改进方案，如对语言模型的，融合知识图谱，多任务学习+预训练语言模型等。

### 1. 预训练 + 知识图谱

预训练诞生之后，在自然语言处理领域的确有了很大的提升，尤其是在阅读理解领域，完全超过了人类的表现，**虽然这并不表示真正的智能，但是依旧意味着，NLP已经逐渐走向成熟。**

随之而来的问题十分明显，如何表示知识`(这句话真抽象，见人说人话，见鬼说鬼话)`，有没有一种方式能够令**大规模语料+预训练语言模型**使模型学习到知识，从而应用到下游任务中。相信这个课题将是接下来一个十分核心的热点，百度和清华就这方面做出了探讨，具体参照：[Bert 改进： 如何融入知识](https://zhuanlan.zhihu.com/p/69941989)

**百度的文章中提出mask掉实体来获取实体的表示，而不仅仅是一个token，可以肯定的是，这样是能更好的表示实体信息（这种“语句增强”的方式要学学，一个意思来回重复说个不停），但是对于实体关系的把握，我个人觉得存疑，这是因为mask操作往往不仅mask掉一个实体，那么别mask掉的实体之间的关系如何把握？**

我个人觉得可以设计一个精巧的任务来验证实体之间的关系，**可以通过知识图谱来生成一个语料，如：**

```
谢霆锋是张柏芝的______。  A.丈夫 B.前夫 C.其它
```

通过预测空白处的位置来验证，这点需要根据具体的知识图谱而定。

清华的那篇文章，**首先将实体与实体之间的关系编码为一个向量，然后把向量融合到预训练语言模型进行训练**，而

实际的操作更加复杂，我个人觉得，这条路恐怕不是正确的道路，不符合大道至简的原则`(奥什么卡姆剃刀原理？)`，且任务太多，反而会引入噪声（个人对知识图谱研究不深，只是直观感觉）。

### 2. 预训练 + 自然语言生成

这部分包括两个课题：

* **如何将BERT用于生成任务**
* **如何设计一个合适于生成任务的语言模型**

前面在 AR 与 AE 模型中已经介绍过为何 BERT 不适用于生成任务中， 那么随之而来的问题就是，既然预训练语言模型在自然语言理解中如此成功，那么我们怎么将其迁移到自然语言生成中呢， 这是一个很大的问题，个人觉得还需要1年以上的时间发展才能出现类似 Bert 这样的突破`(这是怎么感觉出来的😂)`。

我前期看了两篇文章，大致提了一下思路：[Bert 之后：预训练语言模型与自然语言生成](https://zhuanlan.zhihu.com/p/70663422)

**首先，对于第一个问题：如何将BERT用于生成任务，首选Encoder-Decoder架构，Encoder输入原句子，Decoder生成新句子，那么问题在于Encoder和Decoder如何表示？**

**对于Encoder端，我们只需要将BERT直接初始化就行，对于Decoder端，也采用Bert初始化吗？要注意的是，Decoder是用来生成句子的，如果你的embedding信息是通过AE模型训练得来的**`(为什么只考虑Emdedding？不是和训练模型也有很大关系吗？比如是用AE还是AR)`，**那么生成效果可能会诡异的一批。那么现在的问题变成了如何合理的初始化Decoder端的embedding信息呢？**

然后，我们再来谈谈第二个问题：如何设计一个适合生成任务的语言模型？目前看到的两篇文章有两个思路：

* **MASS通过mask连续一小段来试图让模型即学习到理解知识，有学习到生成知识，**通过预测一段连续的tokens的确有助于提升模型生成方面的能力，但我个人认为mask一小段信息所提升的生成功能十分有限，且这回影响到模型理解方面的能力。
* **UULM就很厉害了，它涉及一组语言模型：Unidirectional LM, Masked Bidirectional LM, Seq2Seq LM**，真的是有钱就是任性，但这样直接堆叠语言模型的方式真的好吗？可以肯定的是，不同语言模型的结合必然是接下来的一大趋势，但你这样直接堆是不是有点暴力啊，我个人感觉一般。

老宋个人理解有以下两点：

* 生成是基于理解的，而非独立的，在大脑中，理解与生成式两个区域，先理解后生成，这才是正确的路。

* 理解的归理解，不断提高预训练语言模型在理解领域的表现，对于生成，采用Encoder-Decoder框架。从预训练的角度来说，基于理解层面的训练得到的模型，然后分别初始化Encoder-Decoder端，然后去预训练Decoder端的参数，Freeze/not Freeze Encoder端的参数，从而得到词在Encoder与Decoder的不同Embedding，然后在生成任务中分别使用这两种embedding。

### 3. 预训练 + 多任务学习

多任务学习就更好玩了，目前主要有两大代表：**MT-DNN（Multi-task Deep Nerual Network）和ERNIE2.0**

* **MT-DNN又叫做联合学习，其实就是将预训练模型用在多个任务中去接着训练，从而提高模型的泛化能力。具体来说，训练过程就是把所有任务数据合并在一起，每个batch只有单一任务的数据，同时会带有一个task-type来标识任务类别，然后shuffle之后进行训练。**
* **ERNIE提出了一个很好的训练思路：Continual Learning。这点很有意思，就像人类做题一样，它不像MT-DNN那样，而是这样：**

```
task1 -> task1, task2 -> task1, task2, task3
```

即在训练后面的任务的时候，前面的任务同时参与训练，主要是希望在学习后续任务的时候依旧记得前面任务的学习成果。

我个人认为ERNIE更符合我们人类的训练方式，不过具体的两种学习方式的表现还需要对比一下。

回想我们人类的学习方式，其最初是专题训练，即每个task分别训练，然后在进行总体训练，即所有task一起进行训练，然后发现自己的弱点，然后适当加强对某个任务的训练，然后又进行总体训练，如此反复。

如果要保证训练新任务时不会过分忘记前面所学习到的东西，似乎各个任务的训练样本比例以及训练时间更加重要。

因此。我个人觉得，**联合训练+Continual Learning**是个不错的思路。

### 4. 改进的语言模型

说起改进的语言模型，首推**XLNet**，前段时间也刷了榜，`通过交换token位置来解决mask所带来的预训练与微调不匹配的问题`，这似乎比BERT更加优秀了。

但是从最近的实验结果看，似乎又不是那么回事，XLNet精巧的语言模型设计有没有超越BERT，目前学术界还没有一个定论，**RoBERTa的出现似乎验证了在同等数据集下，XLNet并不占优势，通过精调模型参数，RoBERTa获得了十分漂亮的结果。而XLNet对此给予回击，又在同等条件下对比了XLNet与BERT模型，又说明XLNet的效果的确要超过BERT**，emmm,我也不知道该相信哪个，反证我都会试试，哪个好用哪个。

### 5. 预训练 + 中文领域

十分推荐：[BERT-WWM](<https://github.com/ymcui/Chinese-BERT-wwm>)

对于中文领域，分词还是分字一直是个问题。

BERT选择了分字这条路，**ERNIE通过融入知识**`(怎么融入知识的？)`，**带来了部分分词的效果，**那么在预训练模型中，分词到第有没有用？BERT-WWM给出了答案。

**通过采用mask全词的方式，在原有的BERT-base上接着训练，这其实就是中 字 + 词 级别的组合方式，**我在[深度学习时代，分词真的有必要吗](<https://zhuanlan.zhihu.com/p/66155616>) 中就提到了字级与词级之间的差别，而预训练模型很多能很好地组织二者，的确是件大喜事。

而事实证明，BERT-WWM在中文任务上的确有着优势所在，至少目前开来，**我们中文预训练模型有三大选择了：BERT, ERNIE, BERT-WWM。**

### 6. 预训练 + 精细调参

通过精细调参，BERT能够发挥出更大的威力，RoBERTa证明这一点。

此外，**RoBERTa认为NSP不仅不能带来下游任务的性能提升，反而会有所损害。RoBERTa的出现说明了BERT本身还有很多潜力要挖。**

总的来说，这篇文章依旧是个苦力活，虽然创新一般，但价值很高。

### 7. 预训练 + 基础单元

大多数预训练模型的基础单元是Transformer，那么Transformer有没有改进的空间呢？必然是有的。

**XLNet采Transformerxl作为基础单元来解决长文本问题**`(为什么可以解决？具体改进方法)`，**Transformerxl实际上就是Transformer + 循环机制，这样会带来并行性上的损失。**

相信后续还会有更多的变体来解决transformer的各种问题。

## Reference

[1]BERT:Pre-training of Deep Bidirection Transformers for Language Understanding

[2]ERNIE - Enhanced Language Representation with Informative Entities

[3]ERNIE - Enhanced Representation through Knowledge Integration

[4]ERNIE2.0 - A Continual Pre-training Framework for Language Understanding

[5]MASS - Masked Sequence to Sequence Pre-training for Language Generation

[6]RoBERTa - A Robustly Optimized BERT Pretraining Approach

[7]UNILM - Unified Language Model Pre-training for Natural Language Understanding and Generation

[8]XLNet - Generalized Autoregressibe Pretraining for Language Understanding