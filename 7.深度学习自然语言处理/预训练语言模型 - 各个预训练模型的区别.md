[toc]

# 各个预训练模型的区别

## Reference

[1] [RoBERTa、ERNIE2和BERT-wwm-ext](https://zhuanlan.zhihu.com/p/75987226)

[2] [nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)](https://zhuanlan.zhihu.com/p/76912493)：必看

[3] [BERT：NLP上的又一里程碑](https://zhuanlan.zhihu.com/p/46887114)

[4] [谷歌BERT模型深度解析](https://www.jiqizhixin.com/articles/2018-12-03)



![](https://mmbiz.qpic.cn/mmbiz_png/hGmVcYykAv0BRiadBjEZF8dEp1VqyWWgm3V0YeYaJpFHr2wu0D5M4NXibXP3fbLwb4EWZqmf86J8HNFVsML9ObKA/640?wx_fmt=png)

![](https://pic2.zhimg.com/80/v2-4d60ba6ff919d9cef853f35fc3a9be55_720w.jpg)

![](https://pic3.zhimg.com/80/v2-7734b2580b943573685b9477c2a9e9be_720w.jpg)

![](https://pic4.zhimg.com/v2-0e78a280939451bef50bc0b1a521c45b_1200x500.jpg)

以下按照时间线进行回顾（有些时间相近的，没有严格区分前后顺序）

## ELMO

- 要点：

- - 引入双向语言模型，其实是2个单向语言模型（前向和后向）的集成；
  - 通过保存预训练好的2层biLSTM，通过特征集成或finetune应用于下游任务；

- 缺点：

- - 本质上为自回归语言模型，只能获取单向的特征表示，不能同时获取上下文表示；
  - LSTM不能解决长距离依赖。

- 为什么不能用biLSTM构建双向语言模型？

- - 不能采取2层biLSTM同时进行特征抽取构建双向语言模型，否则会出现**标签泄漏**的问题；**因此**ELMO前向和后向的LSTM**参数独立**，共享词向量，独立构建语言模型；

![](https://pic4.zhimg.com/80/v2-5a777eab37eeb545841af86ea0a4b97f_720w.jpg)

## GPT

- GPT1.0[[10\]](https://zhuanlan.zhihu.com/p/76912493#ref_10)要点：

- - 采用Transformer进行特征抽取，首次将Transformer应用于预训练语言模型；
  - finetune阶段引入语言模型辅助目标（辅助目标对于大数据集有用，小数据反而有所下降，与SiATL相反），解决finetune过程中的灾难性遗忘；
  - 预训练和finetune一致，统一2阶段框架；

- GPT2.0[[11\]](https://zhuanlan.zhihu.com/p/76912493#ref_11)要点：

- - 没有针对特定模型的精调流程：GPT2.0认为预训练中已包含很多特定任务所需的信息。
  - 生成任务取得很好效果，使用覆盖更广、质量更高的数据；

- 缺点：

- - 依然为单向自回归语言模型，无法获取上下文相关的特征表示；

## BERT（自己总结的）

（概括）语言模型采用了MLM，特征抽取器为Transformer，使用双向作为上下文表征，最大的亮点是MLM获取上下文相关的双向特征表示。

（稍微细节，至少要介绍概括里提到的内容）bert采用预训练+微调的方式，在预训练阶段，训练了两个任务，分别是MLM（Mask Language Model）与NSP（下一句预测），MLM指的是随机掩盖一些token，从而通过上下文预测这些token在词表中对应的id。NSP是指给定两个句子，判断是否存在上下句关系。

（缺点）由于预训练使用了MASK，从而导致了两个缺点：微调的时候从未看到[MASK]token，导致预训练与微调不一致，为了缓解这个问题，bert并不是总用"mask"去替换被“mask”的单词，相反，训练数据生成器随机选择15%的token进行处理，其中80%的时间，用mask标记替换单词，10%的时间用一个随机的单词替换该单词，10%的时间保持单词不变。因为transformer encoder不知道被要求预测哪些单词或哪些词被替换了，因此它被迫保持每个输入token的分布式上下文表示，此外，因为随机替换只发生在所有token的1.5％（即15％的10％），这似乎不会损害模型的语言理解能力。

使用MLM的第二个缺点是每个batch只预测了15％的token，这表明模型可能需要更多的预训练步骤才能收敛。团队证明MLM的收敛速度略慢于 left-to-right的模型（预测每个token），但MLM模型在实验上获得的提升远远超过增加的训练成本。

此外被mask的词之间的关系并没有被考虑，即独立条件假设（见贪心科技视频：从bert到xlnet第30分钟）

## XLNET

贪心科技公开课：从bert到XLNERT(讲的很仔细，结合了具体的实例与数学公式)

![](../img/xlnet.png)



 [nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)](https://zhuanlan.zhihu.com/p/76912493) 六、XLNet的内核机制探究



## Roberta

（概括）roberta是在bert基础上，进行精细的调整，依然采用MLM语言模型，使用Transformer作为特征抽取器，使用双向上下文特征表示，最大的亮点是精细调参，舍弃NSP。

（稍微细节）在预训练阶段，舍弃了NSP任务，使用更多的语料和更大的步长。

（缺点）



- 丢弃NSP，效果更好；
- 动态改变mask策略，把数据复制10份，然后统一进行随机mask；
- 对学习率的峰值和warm-up更新步数作出调整；
- 在更长的序列上训练： 不对序列进行截短，使用全长度序列；

## ERNIE

### ERNIE 1.0(百度)[[17\]](https://zhuanlan.zhihu.com/p/76912493#ref_17)(针对BERT原生模型，后续的BERT系列模型是如何引入【知识】的？)

- 在预训练阶段引入知识（实际是预先识别出的实体），引入3种[MASK]策略预测：

- - Basic-Level Masking： 跟BERT一样，对subword进行mask，无法获取高层次语义；
  - Phrase-Level Masking： mask连续短语；
  - Entity-Level Masking： mask实体；

### ERNIE (THU)[[18\]](https://zhuanlan.zhihu.com/p/76912493#ref_18)(针对BERT原生模型，后续的BERT系列模型是如何引入【知识】的？)

![](https://pic2.zhimg.com/80/v2-faaa66f6d2c0ce8fa9998f21e97dc691_720w.jpg)

- 基于BERT预训练原生模型，将文本中的实体**对齐**到外部的知识图谱，并通过知识嵌入得到实体向量作为ERNIE的输入；
- 由于语言表征的预训练过程和知识表征过程有很大的不同，会产生两个独立的向量空间。为解决上述问题，在有实体输入的位置，将实体向量和文本表示通过**非线性变换进行融合**，以融合词汇、句法和知识信息；
- 引入改进的预训练目标 **Denoising entity auto-encoder** (DEA)：要求模型能够根据给定的实体序列和文本序列来预测对应的实体；

### ERNIE 2.0 (百度)[[21\]](https://zhuanlan.zhihu.com/p/76912493#ref_21)(针对BERT原生模型，后续的BERT系列模型是如何引入【多任务学习机制】的？)

![](https://pic1.zhimg.com/80/v2-f9f5c4046ef6a1afa829cb3f726d477c_720w.jpg)

- MTDNN是在下游任务引入多任务机制的，而ERNIE 2.0 是在预训练引入多任务学习（与先验知识库进行交互），使模型能够从不同的任务中学到更多的语言知识。

- 构建多个层次的任务全面捕捉训练语料中的词法、结构、语义的潜在知识。主要包含3个方面的任务：

- - 词法层面，word-aware 任务：捕捉词汇层面的信息，如英文大小写预测；
  - 结构层面，structure-aware 任务：捕捉句法层面的信息，如句子顺序问题、句子距离问题；
  - 语义层面，semantic-aware 任务：捕捉语义方面的信息，如语义逻辑关系预测（因果、假设、递进、转折）；

- 主要的方式是构建**增量学习（**后续可以不断引入更多的任务**）**模型，通过多任务学习**持续更新预训练模型**，这种**连续交替**的学习范式**不会使模型忘记之前学到的语言知识**。

- - - 将3大类任务的若干个子任务一起用于训练，引入新的任务时会将继续引入之前的任务，防止忘记之前已经学到的知识，具体是一个**逐渐增加任务数量**的过程[[22\]](https://zhuanlan.zhihu.com/p/76912493#ref_22)：
      (task1)->(task1,task2)->(task1,task2,task3)->...->(task1，task2,...,taskN)

## BERT-wwm-ext

BERT-wwm-ext主要是有两点改进：

1）预训练数据集做了增加，次数达到5.4B；

2）训练步数增大，训练第一阶段1M步，训练第二阶段400K步。

在一些中文任务上效果稍微有提升