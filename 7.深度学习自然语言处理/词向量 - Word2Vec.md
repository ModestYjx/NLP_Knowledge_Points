[TOC]

# 词向量 - Word2Vec

## 什么是词向量？

词向量就是指表示字或者词表示的数学向量，通过对大量文本的学习，将字或词映射为固定维度的向量，从而用向量之间的关系表示字或词之间的关系，如两个相近的词映射以后的词向量在空间位置上就比较接近。

大致做法是：字或词 -> 一个整数 -> [x1, x2, ..., xn]（n表示向量维度，是一个超参数）

## 什么是Word2Vec[3]?

**Word2Vec是Google发布的一个工具， 用于训练词向量，其提供了两种语言模型（分别是CBOW与Skip-Gram）来供选择， 且Google 基于大规模语料集上训练出了预训练词向量来供开发者或研究者使用。 一般情况下，我们是没有必要自己去训练词向量的，但如果要求特殊，且语料集庞大，自己训练也是可以的。**

Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。

## CBOW模型

CBOW的全称是continue bag of word，中文叫做**连续**词袋模型：以上下文来预测当前词。

![img](https://pic4.zhimg.com/v2-27f3e577618f84c0026968d273d823ef_b.jpg)

如上图所示，是一个两层神经网络，其实在训练过程中考虑到效率问题，常常采用**浅层**神经网络来训练，并取第一层的参数如上图$W_{V\times N}$来作为最终的词向量矩阵（参考 [语言模型：从n元模型到NNLM](https://zhuanlan.zhihu.com/p/43453548)）

CBOW模型的目标是预测 $P(w_t| w_{t-k}, \cdots, w_{t-1}, w_{t+1}, \cdots, w_{t+k}) $

### 1. 前向传播过程

* 输入层：设输入C个单词： $x_{1k}, \cdots, x_{Ck} $，$x_{ik}$用**One-hot**编码表示，是一个V维的一维向量，其中V表示词表长度。

* 输入层到隐藏层：**共享矩阵为**$W_{V \times N}$，其中表示词表长度，N表示最终词向量的维度，W的每一行对应一个N维向量。在隐藏层中，**我们将所有输入的词转化为对应的词向量，然后累加并求平均值**，这样我们就得到了隐藏层的输入（**注意：隐层中并没有使用激活函数，也就是说这里是线性组合**）。其中，h是一个N维的向量。

  $$h = \frac{1}{C} W^T(x_1 + x_2 + \cdots + x_c)$$

* 隐藏层到输出层：隐层的输出为N维向量 $h$ ， 隐层到输出层的权重矩阵为  $W'_{N \times V}$ 。然后，通过矩阵运算我们得到一个 $V \times 1 $ 维向量
  $$
  u = W'^{T} * h
  $$
  

其中，向量 $u$  的第 $i$  行表示词汇表中第 $i$  个词的可能性，然后我们的目的就是取可能性最高的那个词。因此，在最后的输出层是一个softmax 层获取分数最高的词，那么就有我们的最终输出：
$$
P(w_j| context)  =y_i =  \frac{exp({u_j})}{\sum_{k \in V} exp({u_k})}
$$

### 2. 损失函数

我们假定j是被预测的真实单词在词汇表中的下标，那么根据极大似然估计，则**目标函数**的定义如下：
$$
E=-logp(W_{O}|W_I)=-log\frac{exp(u_j)}{\sum_{k \in V}exp(u_k)}=log(\sum_{k \in V}exp(u_k))-u_j
$$

## Skip-gram模型

Skip-Gram的基本思想是：已知当前词 $w_t$ 的前提下，预测其上下文 $w_{t-i}, \cdots , w_{t+i}$ ，与CBOW模型正好相反，模型如下图所示：

![img](https://pic2.zhimg.com/v2-42ef75691c18a03cfda4fa85a8409e19_b.jpg)

<center>图1 skip-gram前向传播过程</center>

![](https://pic2.zhimg.com/80/v2-5c16aa8eaa670485ed5cbcd68e4b8b41_720w.png)

<center>图2 skip-gram前向传播过程(这个图更加详细一些)</center>

注：假设有10000个单词，词向量的维度为300，则前项传播过程如上图。

* 输入层：one-hot表示

* 输入层到隐层：
  $$
  h = W^T * x_k = v^{T}_{w_I}
  $$

* 隐层： 而隐层中没有激活函数，也就是说输入=输出，因此隐藏的输出也是 $h$ 。

* 隐层到输出层：

  一个计算实例如下，其中output weights for car是$W'_{N\times V}$的一列，对应词car的位置索引。
  
  ![](https://pic4.zhimg.com/80/v2-4bfcb484d9353ef612c1a6154f0c3077_720w.png)
  
  （`图片中的softmax函数的分子分母具体是指什么？`）

   * 首先，因为要输出C个单词（C的选取应该指的就是窗口大小的选取，见下图，选取的窗口为2时，一个句子所产生的的训练样本），因此我们输出有C个分布（`是指有C个W'矩阵吗？，还是只有一个W'矩阵？`）：$y_1, \cdots y_C $，且每个分布都是独立的，我们需要单独计算， 其中 $y_i$  表示窗口的第 $i$  个单词的分布。
  
     ![](https://pic4.zhimg.com/80/v2-ca21f9b1923e201c4349030a86f6dc1f_720w.png)
  
   * 其次， 因为矩阵 $W'_{N \times V}$ 是共享的，因此我们得到的 $V \times 1$ 维向量 $y$ 其实是相同的，也就是有 $y_{c,j} = y_j$ ，这里$y$的**每一行同 CBOW 中一样，表示的也是评分。**
  
   * 最后，每个分布都经过一个 softmax 层，不同于 CBOW，我们此处产生的是第 $i$ 个单词的分布（共有C个单词）（`不是应该有V个单词吗？C与V在计算上的关系是什么？`），如下：
     $$
     P(w_{i,j}| context)  =y_i =  \frac{exp({u_j})}{\sum_{k \in V} exp({u_k})}
     $$



### 2. 损失函数

假设 $j^*$ 是真实单词在词汇表中的下标，那么根据极大似然法，则目标函数定义如下：
$$
\begin{split} E &= - log \, p(w_1, w_2, \cdots, w_C | w_I)   \\ &= - log \prod_{c=1}^C P(w_c|w_i) \\ &= - log  \prod_{c=1}^{C} \frac{exp(u_{c, j})}{\sum_{k=1}^{V} exp(u_{c,k}) } \\ &= - \sum_{c=1}^C u_{j^*_c} + C \cdot log \sum_{k=1}^{V} exp(u_k) \end{split}
$$

## 模型复杂度

本节中我们来分析一下模型训练时的复杂度，无论是在CBOW还是Skip-Gram模型中，都需要学习两个词向量矩阵： $W, W'$ 。

对于矩阵 $W$ ， 从前向传播中可以看到， 可以看到对于每一个样本(或mini-batch)，CBOW更新 $W$  的 C 行（h与C个x相关）， 而Skip-Gram 更新W中的其中一行（h与1个x相关），这点训练量并不算大。

对于 $W'$  而言， 无论是 CBOW 还是 Skip-Gram 模型，每个训练样本(mini-batch)都更新 $W'$ 的所有 $V \times N$ 个元素。

（`上面这段话没有看懂？`）

为了解决训练速度慢的问题，Word2Vec中提供了两种策略来进行优化。

## Hierarchical Softmax, Negative Sampling

### 1. Hierarchical Softmax

HS基于哈夫曼树将计算量大的部分转化成二分类问题。

![hs](..\img\word2vec\hs.png)

原来的模型中，在隐层之后通过W'连接输出层（举个栗子，我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的**输入-隐层权重矩阵**和**隐层-输出层的权重矩阵**都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢（`为什么？`）的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。），现在HS去掉了W'，而是在隐层后面直接连接一个哈夫曼树，隐层的输出对应哈弗曼树的根节点，树的V个叶子节点对应词表中的V个单词，从根节点到叶子节点的路径对应着一个选择，我们的目标选择概率最大的路径，从而将问题转化为了使正确输出的路径的概率最大，即$P(w=w+path|w_i)$最大。

设$n(w_i, j)$表示从root到叶子节点$w_i$的路径上的第i个节点，对于每个非叶子节点，都对应着一个n维（隐层神经元的个数）的向量$v_{i, j}$，使用sigmoid函数$\sigma (x)=\frac{1}{1+exp(-x)}$，其中$x=v_{i,j}*h,h为隐层输出向量$,来判断向左还是向右走，那么第 n 个节点向左以及向右的概率分别为：
$$
P(n,left) = \sigma(v_{i,j} \cdot h) \\
P(n, right) = 1 - \sigma(v_{i,j} \cdot h)
$$
于是就有
$$
P(w=w_O|w_i) = \prod_{j=1}^{L(w)-1} P(\sigma(I(n(w, j+1 == left) v_w' \cdot h))\eqno(1)
$$

* $I()$ ：指示函数，条件成立值为1， 反之为 -1
* $L(w)$ ：表示整条路径的长度

这样我们就能够通过训练来更新每个非叶子节点的参数 $v_w'$了。举例来说，图上加黑的黑色路径： $(n(w_2,1),n(w_2,2),n(w_2,3),w2$， 对于一个训练样本，我们要使得 $P(w_O=w_2|w_I)$  概率最大：
$$
P(w_2=w_O) = P(n(w_2, 1), left) \cdot P(n(w_2, 2), left) \cdot P(n(w_2, 3), right)
$$
且需要注意的时，再一个非叶子节点处， 向左向右的概率和为1， 因此一直分裂下去，最后的和肯定还是1， 因此可以得出：
$$
\sum_{j=1}^V P(w_j = w_O) = 1
$$
损失函数同样为最大似然：
$$
E=-logP(W=W_O|W_I) 参照上式（1）
$$
通过Hierarchical Softmax，隐层到输出层的计算复杂度从O(V)降到了O(logV)。

### 2. Negative Sampling -- 负采样

当词表很大的时候（比如bert的词表有2万多条数据），在输出层中，由于每次要通过softmax函数从整个词表中找出概率最大的词，计算效率就很低，那我们可不可以将候选项的词的数量降低一些呢？负采样采用的就是这样思想：**不直接让模型从整个词表中找最可能的词，而是直接给定这个词（正例）和几个随机采样的噪声词（负例），然后模型能够从这几个词中找到正确的词，就算达到目的了。**

那噪音词怎么选取呢？作者使用**基于词频的权重分布**来获得概率分布进行抽样：
$$
weight(w) = \frac{count(w)^{0.75}}{\sum_u count(w)^{0.75}}
$$
相比于直接使用词频作为权重项，取0.75幂的好处是可以减弱不同词的词频差异过大带来的影响，使得小词频的词被采样的概率变大。

此时的损失函数为：
$$
E = - log \sigma(v_{w_O}' h) - \sum_{w_j \in W_{neg}} log \sigma(-v_{w_j}'h)
$$




## Reference

[1] [理解 Word2Vec 之 Skip-Gram 模型](https://zhuanlan.zhihu.com/p/27234078)：结合具体的例子讲解

[2] Mikolov, T.(2013). Distributed Representations of Words and Phrases and their Compositionality.

[3] Mikolov, T.(2013). Efficient Estimation of Word Representations in Vector Space.

[4] Rong, X.(2014).word2vec Parameter Learning Explained.

[5] [Word2vec数学原理全家桶](https://shomy.top/2017/07/28/word2vec-all/)：其中的C个分布还是没看懂，暂且不管