# 改进NLP模型的一些思路

## 1. 快速构建baseline

构建一个有效的baseline能帮助我们**了解数据的质量和确定模型构建的方向**

### 1. 分析数据

* 数据集规模
* 训练集、验证机、测试集的数据分布，样本数量，样本分布
* 是否存在缺失值

### 2. 寻找合适的模型

### 3. 简单模型，简单baseline

初始模型的作用在于迅速了解数据质量和特点，所以模型的性能通常不需要达到很高，模型复杂度也不需要很高。当然，这不是绝对的，甚至可以拿bert等与训练模型作为baseline。

### 4. 剖析模型

一旦确定了一个初始模型，当面对一批新的数据时，要重新去认识这个模型，观察模型内部发生了什么。

解剖模型一般要注意误差的变化、训练和验证机的差异，出现一些NAN或者INF等情况时，需要打印内部输出，确定问题出现的时间和位置；在完成训练后，结合数据背景测试模型的输出是否合理。

## 2. 改进模型

### 1. 数据角度

数据决定模型的上限。

### 2. 模型角度

在数据量充足的情况下，对同类型的模型，增大模型规模来提升性能无疑是最直接和有效的手段。但越大的参数模型优化也会越难，所以要在合理的范围内对模型进行参数规模的修改。而不同类型的模型，在不同数据上的优化成本都可能不一样，所以在探索模型是需要尽可能挑选油画简单，训练效率更高的模型进行训练。

### 3.调参优化角度

如果你已经知道模型的性能为什么不再提高了，那已经向提升性能跨出了一大步。超参数调整本身是一个比较大的问题。一般可以包含模型初始化的配置，优化算法的选择，学习率的策略以及如何配置正则和损失函数等。这里需要指出的是对于同一优化算法，相近参数规模的前提下，不同类型的模型总能表现出不同的性能。这实际上就是模型优化的成本。从这个角度的反方向来思考，同一模型也总能找到一个比较合适的优化算法，所以确定了模型后选择适合模型优化算法也是非常重要的手段。

### 4.训练角度

很多时候我们会把优化和训练同时进行。在这里分开将是为了强调要进行充分的训练。在越大规模的数据集或者模型上，诚然一个好的优化算法总能加速收敛。但在未探索到模型的上限之前，永远不知道训练多久算训练完成。所以在改善模型上充分训练是必要的过程。充分训练的含义不仅仅是增大训练轮数。有效的学习率衰减和正则同样是充分训练的必要手段。