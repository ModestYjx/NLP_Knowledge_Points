[TOC]



# 机器学习：逻辑回归

logistic回归简介

logistic回归的数学表达式

如何求解最优的 $\theta$

常见问题

1.逻辑回归与线性回归

2.推导一下LR

3.LR如何实现多分类

4.LR为何要对特征进行离散化

5.逻辑回归中，增大L1正则化会是什么结果

`疑问：和极大似然函数有什么关系，使用梯度下降更新损失函数不就可以了吗？`

## logistic回归简介

logistic回归是一种分类算法，**其基本思想是：根据现有数据对分类边界线建立回归方程（[“回归”的意义](https://www.zhihu.com/question/30123729)），以此进行分类**。需要注意的是，logistic回归是对分类边界的拟合，而不是对所有数据点进行拟合（这种拟合方式是我们下面所提到的线性回归）。

## logistic回归的数学表达式

$$
h_\theta(x)=sigmoid(\theta^TX)=\frac{1}{1+e^{-\theta^TX}}
$$

## 如何求解最优的 $\theta$

在logistic回归中，选择交叉熵损失函数：
$$
Cost(h_\theta(x),y)=\begin{cases}-log(h_\theta(x))&if \,y=1\\-log(1-h_\theta(x))&if \,y=0\\\end{cases}
$$
假设有m个样本，总的损失为：
$$
J(\theta)=-\frac1m\left[\sum_{i=1}^my^ilog(h_\theta(x^i))+(1-y^i)(log(1-h_\theta(x^i)))\right]
$$
为了防止模型过拟合，再增加一个正则化项：
$$
J(\theta)=-\frac1m\left[\sum_{i=1}^my^ilog(h_\theta(x^i))+(1-y^i)(log(1-h_\theta(x^i)))\right]+\frac\lambda{2m}\sum_{j=1}^m\theta_j^2
$$
然后对$J(\theta)$求一次导，运用梯度下降的方法求解$J(\theta)$的最小值对应的参数

## 常见问题

### 1.逻辑回归与线性回归

区别：

* 逻辑回归处理的是分类问题，线性回归处理的是回归问题

* 线性回归的拟合函数本质是对**输出变量y的拟合**，逻辑回归是对**标签为1的样本的概率**的拟合。

  线性回归：$f(x)=\theta^Tx$

  逻辑回归：$f(x)=P(y=1|x;\theta)=g(\theta^Tx),\,g(x)=\frac1{1+e^{-z}}$

* 线性回归其参数计算方式为**最小二乘法**，`逻辑回归的参数更新方式为极大似然函数`

* 线性回归容易受到异常值的影响，逻辑回归对异常值有较好的稳定性。

### 2.推导一下LR

sigmoid函数：
$$
g(z) = \frac{1}{1+e^{-z}} \\
g'(x)=g(x)(1-g(x))\\
$$
LR的定义：
$$
h_\theta(X)=g(\theta^TX)=sigmoid(\theta^TX)=\frac{1}{1+e^{-\theta^TX}}\\
$$

`LR满足伯努利分布：`

`损失函数（极大似然）：`

`参数优化（梯度上升）`

### 3.LR如何实现多分类

* 方法1.假设有n个类别，就设计n个逻辑回归分类器。根据每个类别建立一个二分类器，本类别的样本标签值为1，其它类别的样本标签值为0。

* 方法2.`修改逻辑回归的损失函数（如多分类交叉熵）`,使用softmax函数构造模型解决多分类问题，softmax分类模型会有样本标签类别数目个输出，每个输出对应该样本属于各个类别的概率。

  `若所有类别之间存在明显的互斥关系，则使用softmax分类模型，否则使用多逻辑回归分类器。`

### `4.LR为何要对特征进行离散化`

### 5.逻辑回归中，增大L1正则化会是什么结果

为是总损失保持较小值，所有参数$\theta$会趋近于0。

