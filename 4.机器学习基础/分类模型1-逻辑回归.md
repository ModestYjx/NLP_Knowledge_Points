[TOC]

# 机器学习：逻辑回归

logistic回归简介

logistic回归的数学表达式

如何求解最优的 $\theta$

常见问题

1.逻辑回归与线性回归

2.推导一下LR

3.LR如何实现多分类

4.LR为何要对特征进行离散化

5.逻辑回归中，增大L1正则化会是什么结果

## logistic回归简介

logistic回归是一种分类算法，**其基本思想是：根据现有数据对分类边界线建立回归方程（[“回归”的意义](https://www.zhihu.com/question/30123729)），以此进行分类**。需要注意的是，logistic回归是对分类边界的拟合，而不是对所有数据点进行拟合（这种拟合方式是我们下面所提到的线性回归）。

## logistic回归的数学表达式

$$
h_\theta(x)=sigmoid(\theta^TX)=\frac{1}{1+e^{-\theta^TX}}
$$

注：偏置项b可以通过变换放到wx里面，即$wx+b -> w'x$

## 如何求解最优的 $\theta$

在logistic回归中，选择交叉熵损失函数：
$$
Cost(h_\theta(x),y)=\begin{cases}-log(h_\theta(x))&if \,y=1\\-log(1-h_\theta(x))&if \,y=0\\\end{cases}
$$
假设有m个样本，总的损失为：
$$
J(\theta)=-\frac1m\left[\sum_{i=1}^my^ilog(h_\theta(x^i))+(1-y^i)(log(1-h_\theta(x^i)))\right]
$$
为了防止模型过拟合，再增加一个正则化项：
$$
J(\theta)=-\frac1m\left[\sum_{i=1}^my^ilog(h_\theta(x^i))+(1-y^i)(log(1-h_\theta(x^i)))\right]+\frac\lambda{2m}\sum_{j=1}^m\theta_j^2
$$
然后对$J(\theta)$求一次导，运用梯度下降的方法求解$J(\theta)$的最小值对应的参数

## 常见问题

### 1.逻辑回归与线性回归

区别：

* 逻辑回归处理的是分类问题，线性回归处理的是回归问题

* 线性回归的拟合函数本质是对**输出变量y的拟合**，逻辑回归是对**标签为1的样本的概率**的拟合。

  线性回归：$f(x)=\theta^Tx$

  逻辑回归：$f(x)=P(y=1|x;\theta)=g(\theta^Tx),\,g(x)=\frac1{1+e^{-z}}$

* 线性回归其参数计算方式为**最小二乘法**，`逻辑回归的参数更新方式为极大似然函数`

* 线性回归容易受到异常值的影响，逻辑回归对异常值有较好的稳定性。

### 2.推导一下LR

sigmoid函数：
$$
g(z) = \frac{1}{1+e^{-z}} \\
g'(x)=g(x)(1-g(x))\\
$$
LR的定义：
$$
h_\theta(X)=g(\theta^TX)=sigmoid(\theta^TX)=\frac{1}{1+e^{-\theta^TX}}\\
$$

- LR 满足**伯努利分布：**（参考[哈？你还认为似然函数跟交叉熵是一个意思呀？](https://www.jiqizhixin.com/articles/2018-07-12-5)与[逻辑回归 logistics regression 公式推导：五、逻辑回归的损失函数](https://zhuanlan.zhihu.com/p/44591359)）
  $$
  P(Y=1|x; \theta) = h_{\theta}(x) \\
  P(Y=0|x; \theta)  = 1 - h_{\theta}(x) \\
  p(y|x; \theta) = (h_{\theta}(x))^y (1-h_{\theta}(x))^{1-y}
  $$

- **损失函数（极大似然）:**  对于训练数据集，特征数据 $x={x_1, ...x_m}$ 和其对应的分类标签 $y = {y_1,...y_m}$ ， 假设 m 个样本是相互独立的，那么极大似然函数为（参考：[逻辑回归 logistics regression 公式推导：六、最大似然估计MLE](https://zhuanlan.zhihu.com/p/44591359)）： 
  $$
  \begin{align}L(\theta) &= \prod_{i=1}^m p(y^{(i)}|x(i);\theta) \\ &= \prod_{i=1}^m  (h_{\theta}(x^{(i)}))^{y^{(i)}} (1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}\\\end{align}
  $$
  那么它的 log 似然（因为连乘计算复杂，通过转换，降低计算复杂度）为：
  $$
  \begin{align}
  L(\theta) &= log L(\theta ) \\
  &= \sum_{i=1}^m y^{(i)} log h(x^{(i)}) + (1-y^{(i)}) log (1-h(x^{(i)}))
  \end{align}
  $$

- 参数优化（梯度上升）
  $$
  \begin{align}
  \frac{\partial L(\theta)}{\partial \theta_j} &= (y \frac{1}{g(\theta^Tx)} - (1-y) \frac{1}{1 -g(\theta^Tx)}) \frac{\delta g(\theta^Tx)}{\delta \theta_j} \\
  &= (y \frac{1}{g(\theta^Tx)} - (1-y)\frac{1}{1 -g(\theta^Tx)} ) g(\theta^Tx)(1-g(\theta^Tx)) \frac{\delta \theta^Tx}{\theta_j} \\
  &= (y (1 - g(\theta^Tx)) - (1-y) g(\theta^Tx)) x_j \\
  &= [y - h_{\theta} (x)]x_j \\
  \end{align}
  $$




$$
\begin{align}
\theta_j &= \theta_j + \alpha \frac{\partial L(\theta)}{\partial \theta} \\
&= \theta_j + \alpha [y^{(i)} - h_{\theta} (x^{(i)})]x_j^{(i)}   
\end{align}
$$



- 损失函数：
  $$
  J(\theta) = - \frac{1}{m}   \left[  \sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)}))   + (1-y^{(i)}) log(1 - h_\theta(x^{(i)}))             \right]
  $$



### 3.LR如何实现多分类

* 方法1.假设有n个类别，就设计n个逻辑回归分类器。根据每个类别建立一个二分类器，本类别的样本标签值为1，其它类别的样本标签值为0。

* 方法2.`修改逻辑回归的损失函数（如多分类交叉熵）`,使用softmax函数构造模型解决多分类问题，softmax分类模型会有样本标签类别数目个输出，每个输出对应该样本属于各个类别的概率。

  `若所有类别之间存在明显的互斥关系，则使用softmax分类模型，否则使用多逻辑回归分类器。`

### `4.LR为何要对特征进行离散化`

- **非线性。** 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代； 
- **速度快。** 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展
- **鲁棒性。** 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
- **方便交叉与特征组合**： 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。
- **稳定性：** 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。
- **简化模型：** 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

### 5.逻辑回归中，增大L1正则化会是什么结果

为是总损失保持较小值，所有参数$\theta$会趋近于0。

## QA

### 1. 交叉熵与最大似然

* 交叉熵

  [逻辑回归：代价函数](https://zhuanlan.zhihu.com/p/28408516)

* 最大似然

  [逻辑回归 logistics regression 公式推导：六、最大似然估计MLE](https://zhuanlan.zhihu.com/p/44591359)

* [哈？你还认为似然函数跟交叉熵是一个意思呀？](https://www.jiqizhixin.com/articles/2018-07-12-5)

## Reference

[1] [逻辑回归](https://zhuanlan.zhihu.com/p/28408516)

[2] [哈？你还认为似然函数跟交叉熵是一个意思呀？](https://www.jiqizhixin.com/articles/2018-07-12-5)：深度好文！