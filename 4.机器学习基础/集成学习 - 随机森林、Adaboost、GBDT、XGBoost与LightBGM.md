[toc]

# 集成学习 - 随机森林、Adaboost、GBDT、XGBoost与LightGBM

## 阅读提示

如果对集成学习有一定了解的话，直接看[GBDT算法原理深入解析](https://www.zybuluo.com/yxd/note/611571)，不了解的话：先看整体架构[集成学习，包含公式推导与代码实战](https://www.bilibili.com/video/BV1Ca4y1t7DS?p=6)，有个大概的了解，同时xgboost的推导可以看贪心科技的视频，通过公式对比与讲解解释了梯度下降与GBDT的相同之处，以及xgboost与GBDT的不同。

作为补充与更好地理解，再看[GBDT算法原理深入解析](https://www.zybuluo.com/yxd/note/611571)，然后看[【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）](https://zhuanlan.zhihu.com/p/87885678)，如果实在看不下去，可以看一下简单的视频[xgboost详解](https://www.bilibili.com/video/av61936066?p=3)。



## 随机森林

### 基本思想

1. 假如有N个样本，则有放回的随机选择N个样本(每次随机选择一个样本，然后返回继续选择)。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。

2. 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。

3. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
4. 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。

参考：https://zhuanlan.zhihu.com/p/22097796

其中，随机性体现再两点：与bagging不一样，bagging没有“每一次选择分叉特征时，限定为在随机选择的特征的子集中寻找一个特征”

## AdaBoost

### 基本思想

AdaBoost是英文"Adaptive Boosting"（自适应增强）的缩写，它的自适应在于：前一个基本分类器被错误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器。
**Adaboost算法可以简述为三个步骤：**
（1）首先，是初始化训练数据的权值分布D1。假设有N个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值：w1=1/N。
（2）然后，训练弱分类器hi。具体训练过程中是：如果某个训练样本点，被弱分类器hi准确地分类，那么在构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值就应该增大。权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。
（3）最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。
换而言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。

## GBDT

### 基本思想：

GBDT的学习算法：

1. 算法每次迭代生成一颗新的决策树 
2. 在每次迭代开始之前，计算损失函数在每个训练样本点的**一阶导数和二阶导数**（这里似乎写错了，好像是只计算一阶导数） （由于要学习的函数仅仅依赖于目标函数，从经过化简的目标函数可以看出只需为学习任务定义好损失函数，并为每个训练样本计算出损失函数的一阶导数和二阶导数，通过在训练样本集上最小化经过化简的目标函数即可求得每步要学习的函数，从而根据加法模型可得最终要学习的模型。）
3. 通过贪心策略生成新的决策树，令目标函数的一阶导数等于0，计算出每个叶节点对应的预测值 
4. 把新生成的决策树添加到模型中

其中：

单颗决策树的学习过程可以大致描述为： 

1. 枚举所有可能的树结构 
2. 计算当前树对应的分数（目标函数值），分数越小说明对应的树结构越好 
3. 根据上一步的结果，找到最佳的树结构，令目标函数的一阶导数等于0，计算出每个叶节点对应的预测值 

然而，可能的树结构数量是无穷的，所以实际上我们不可能枚举所有可能的树结构。通常情况下，我们采用**贪心策略**来生成决策树的每个节点。 

1. 从深度为0的树开始，对每个叶节点枚举所有的可用特征 
2. 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益） 
3. 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集 
4. 回到第1步，递归执行到满足特定条件为止。

参考：[GBDT算法原理深入解析](https://www.zybuluo.com/yxd/note/611571)

## XGboost

### 基本思想

在GBDT的基本思想上，Xgboost做了一些改进：从而在效果和性能上有明显的提升。

**第一，GBDT将目标函数泰勒展开到一阶，而xgboost将目标函数泰勒展开到了二阶**。保留了更多有关目标函数的信息，对提升效果有帮助。

**第二，GBDT是给新的基模型寻找新的拟合标签**（前面加法模型的负梯度），**而xgboost是给新的基模型寻找新的目标函数**（目标函数关于新的基模型的二阶泰勒展开）。

**第三，xgboost加入了和叶子权重的L2正则化项**，因而有利于模型获得更低的方差。

**第四，xgboost增加了自动处理缺失值特征的策略。**通过把带缺失值样本分别划分到左子树或者右子树，比较两种方案下目标函数的优劣，从而自动对有缺失值的样本进行划分，无需对缺失特征进行填充预处理。

此外，xgboost还支持候选分位点切割，特征并行等，可以提升性能。

### 例子

假设目标值为1000，第一课树预测结果为950，那么残差就为50，然后第二棵树根据之前的残差50预测新的结果为985，然后第三棵树，此时看到的是一个前两棵树组成的整体的结果，即985，然后残差为15，于是就进行新的预测结果为990。

### 优缺点

**优点**

1. **精度更高：**GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；
2. **灵活性更强：**GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；
3. **正则化：**XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；
4. **Shrinkage（缩减）：**相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；
5. **列抽样：**XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；
6. **缺失值处理：**XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；

![image-20200331220215727](..\img\xgboost缺失值处理.png)

7. **可以并行化操作：**块结构可以很好的支持并行计算。

**缺点**

1. 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；
2. 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。



## LightGBM

可以参考：https://zhuanlan.zhihu.com/p/91167170

我们刚刚分析了 XGBoost 的缺点，LightGBM 为了解决这些问题提出了以下几点解决方案：

1. 单边梯度抽样算法；梯度越小说明模型拟合的越好，所以应该关注那些对应梯度大的样本，对梯度小的样本进行抽样即可。

2. 直方图算法；

   ​	连续的特征离散化为 k 个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（含有 k 个 bin）。利用直方图算法我们无需遍历数据，只需要遍历 k 个 bin 即可找到最佳分裂点。

   ​	在构建叶节点的直方图时，我们还可以通过父节点的直方图与相邻叶节点的直方图相减的方式构建，从而减少了一半的计算量。

3. 互斥特征捆绑算法；

   ​	高维特征往往是稀疏的，而且特征间可能是相互排斥的（如两个特征不同时取非零值），如果两个特征并不完全互斥（如只有一部分情况下是不同时取非零值），可以用互斥率表示互斥程度。互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。

4. 基于最大深度的 Leaf-wise 的垂直生长算法；

5. 类别特征最优分割；

6. 特征并行和数据并行；

7. 缓存优化。

### 优缺点

本节主要总结下 LightGBM 相对于 XGBoost 的优点，从内存和速度两方面进行介绍。

**内存更小**

1. XGBoost 使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从 ![[公式]](https://www.zhihu.com/equation?tex=O%282%2A%5C%23data%29) 降低为 ![[公式]](https://www.zhihu.com/equation?tex=O%28%5C%23bin%29) ，极大的减少了内存消耗；
2. LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗；
3. LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。

**速度更快**

1. LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度；
2. LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算；
3. LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量；
4. LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略；
5. LightGBM 对缓存也进行了优化，增加了 Cache hit 的命中率。

## 其它

模型的预测精度由模型的**偏差和方差**共同决定，**损失函数代表了模型的偏差**，**想要方差小则需要简单的模型**，所以目标函数由模型的损失函数 ![[公式]](https://www.zhihu.com/equation?tex=L) 与抑制模型复杂度的正则项 ![[公式]](https://www.zhihu.com/equation?tex=%5COmega) 组成，所以我们有：

![[公式]](https://www.zhihu.com/equation?tex=Obj+%3D%5Csum_%7Bi%3D1%7D%5En+l%28%5Chat%7By%7D_i%2C+y_i%29+%2B+%5Csum_%7Bt%3D1%7D%5Ek+%5COmega%28f_t%29+%5C%5C+)