# 集成学习 - GBDT、XGBoost与LightGBM

---

## Refrence

[0] [https://zhuanlan.zhihu.com/p/34534004](https://zhuanlan.zhihu.com/p/34534004)

[1] [xgboost详解](https://www.bilibili.com/video/av61936066?p=3)

[2] [【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）](https://zhuanlan.zhihu.com/p/87885678)

## GBDT

GBDT ， Gradient Boosting  Decision Tree， 叫 梯度提升决策树。





## XGboost

### 残差

 $\hat{y_i}^{(t-1)}-y_i$：当前模型的真实值与预测值之间的差异，即为残差

模型的预测精度由模型的**偏差和方差**共同决定，**损失函数代表了模型的偏差**，**想要方差小则需要简单的模型**，所以目标函数由模型的损失函数 ![[公式]](https://www.zhihu.com/equation?tex=L) 与抑制模型复杂度的正则项 ![[公式]](https://www.zhihu.com/equation?tex=%5COmega) 组成，所以我们有：

![[公式]](https://www.zhihu.com/equation?tex=Obj+%3D%5Csum_%7Bi%3D1%7D%5En+l%28%5Chat%7By%7D_i%2C+y_i%29+%2B+%5Csum_%7Bt%3D1%7D%5Ek+%5COmega%28f_t%29+%5C%5C+)

![[公式]](https://www.zhihu.com/equation?tex=%5COmega) 为模型的正则项

## 例子

假设目标值为1000，第一课树预测结果为950，那么残差就为50，然后第二棵树根据之前的残差50预测新的结果为985，然后第三棵树，此时看到的是一个前两棵树组成的整体的结果，即985，然后残差为15，于是就进行新的预测结果为990。

## 推导

1. $y_t = y_{t-1}+f_t(x_i)$，其中$y_t$第t轮的模型预测，$f_t(x_i)$表示新加的一个函数
2. 目标：均方误差+叶子数的惩罚项（防止叶子数过多，而导致的过拟合）+正则项

2. 用泰勒展开来近似原来的目标：

## 难点

（个人总结）

1. 目标函数公式推导
2. 选取特征的各种策略

## 优缺点

**1.3.1 优点**

1. **精度更高：**GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；
2. **灵活性更强：**GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；
3. **正则化：**XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；
4. **Shrinkage（缩减）：**相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；
5. **列抽样：**XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；
6. **缺失值处理：**XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；
7. **可以并行化操作：**块结构可以很好的支持并行计算。

**1.3.2 缺点**

1. 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；
2. 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。

## 使用

pip install xgboost



## LightGBM

我们刚刚分析了 XGBoost 的缺点，LightGBM 为了解决这些问题提出了以下几点解决方案：

1. 单边梯度抽样算法；梯度越小说明模型拟合的越好，所以应该关注那些对应梯度大的样本，对梯度小的样本进行抽样即可。

2. 直方图算法；

   ​	连续的特征离散化为 k 个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（含有 k 个 bin）。利用直方图算法我们无需遍历数据，只需要遍历 k 个 bin 即可找到最佳分裂点。

   ​	在构建叶节点的直方图时，我们还可以通过父节点的直方图与相邻叶节点的直方图相减的方式构建，从而减少了一半的计算量。

3. 互斥特征捆绑算法；

   ​	高维特征往往是稀疏的，而且特征间可能是相互排斥的（如两个特征不同时取非零值），如果两个特征并不完全互斥（如只有一部分情况下是不同时取非零值），可以用互斥率表示互斥程度。互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。

4. 基于最大深度的 Leaf-wise 的垂直生长算法；

5. 类别特征最优分割；

6. 特征并行和数据并行；

7. 缓存优化。



### 2.3 与 XGBoost 的对比

本节主要总结下 LightGBM 相对于 XGBoost 的优点，从内存和速度两方面进行介绍。

**2.3.1 内存更小**

1. XGBoost 使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从 ![[公式]](https://www.zhihu.com/equation?tex=O%282%2A%5C%23data%29) 降低为 ![[公式]](https://www.zhihu.com/equation?tex=O%28%5C%23bin%29) ，极大的减少了内存消耗；
2. LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗；
3. LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。

**2.3.2 速度更快**

1. LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度；
2. LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算；
3. LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量；
4. LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略；
5. LightGBM 对缓存也进行了优化，增加了 Cache hit 的命中率。

